---
title: 工作备忘
date: 2025-8-23 12:39:04
tags: [工作备忘, kubectl, linux]
categories:
  - [工作备忘]
---

### kubectl常用命令

<!--more-->

* kubectl get nodes --show-labels：显示标签信息
* kubectl -n devops get sts：查询各个服务的statefulset
* kubectl -n devops edit sts hadop-test-datanode-nodemanager：查看各个服务的statefulset详情，通过第2步查出来各个服务名和服务状态，sts后跟的就是第2步的服务名
* kubectl -n devops get pod -o wide：查看所有pod运行状态，pod ip和所在节点ip（-o wide的作用是显示pod ip和所在节点ip）。在我们的环境中，pod和容器一一对应
* kubectl -n devops describe pods：显示所有pod信息（如果遇到一直pending状态的pod可以执行这个命令定位拉不起的原因）
* kubectl -n devops delete pod ${pod名}：删除pod（和第5步删除服务一起使用，先删pod，再删服务）
* kubectl -n devops logs ${podName}：查看pod/容器日志
* kubectl get ns：获取所有命名空间
* kubectl -n devops exec -it ${podName} /bin/bash：进入pod
* kubectl get nodes：获取所有节点
* kubectl describe node ${nodename}：查看指定节点详情
* kubectl get pvc -A：查看pvc
* kubectl -n yxz-1 describe pvc volume-0-hdp-hadoop-hdp-datanode-nodemanager-0：查看pvc详细信息
* kubectl -n namespace delete pvc ${pvc名}
* kubectl get pod -n devops | grep Evicted | awk '{print $1}' | xargs kubectl delete pod -n devops：批量删除pod
* kubectl get pod –A可以显示所有的pod，无需指定namespace
* kubectl describe node 10.35.26.20 最后会显示该节点已分配资源
* 给节点加标签：kubectl label nodes 10.2.2.123 key1=val1；删除节点标签：kubectl label nodes 10.32.24.11 key-；修改节点标签：kubectl label nodes 10.32.24.11 user.dahuatech.com/DataIntelligence=xxx --overwrite
* kubectl -n kafka-perf-test get service -o wide查看nodeport clusterip等资源

### linux常用命令

#### 抓包命令

tcpdump -i eth0 host 192.168.1.20 port 30950 -w /tmp/temp.cap

#### helm打包命令

helm package ./xxx
如：.\helm.exe package D:\code_git\HDP-Production\hdp-production\hdp_hadoop

渲染sts命令 helm template --debug ./hdp_hadoop

#### 查询操作系统版本信息

cat /etc/os-release   cat /proc/version

#### 分发文件

scp -P 22 Hive_HiveServer2.keytab 10.35.26.50:/home

#### 磁盘相关

查询挂载盘符和使用率：df -ah | grep cloud

查询某个路径下的使用率：du -sh /cloud/data1/kafka

查看磁盘信息：smartctl --info /dev/sda

#### 开启sshd并设置密码

/usr/sbin/sshd &&  echo -e 'DH2021@hdp\nDH2021@hdp' | (passwd root)

#### 创建软链接

ln  -s   [源文件]   [软链接文件]

#### 去除中文乱码

sed -i 's/\r//g' FileName

#### 测试磁盘写速率性能

time dd if=/dev/zero of=/dahuacloud/user/data/data0002/test/test.file bs=1G count=2 oflag=direct

#### iperf测带宽

服务端iperf -s。客户端iperf -c ${serverIP}

#### vi文件快捷操作

dd:删除光标所在的一整行

gg：回到顶部

GG：回到底部

.,$d：删除全部内容

#### pagecache和脏页相关

cat /proc/sys/vm/dirty_background_bytes

cat /proc/sys/vm/dirty_bytes

```
cat /proc/vmstat |grep dirty 
nr_dirty 30301  当前脏页页数
nr_dirty_threshold 786432     对应dirty_bytes：每页4k
nr_dirty_background_threshold 3276对应dirty_background_bytes：每页4k
sysctl -a | grep dirty：可以获取到系统设置的脏页大小
sar -r 1 ：可以获得当前脏页大小的实时数据
```

#### 交互行循环

for i in {1..10}; do sleep 1; df -h | grep data1; sleep 2; done

#### grep指定正则表达式

cat slow.log | grep -E '2023-05-26 17|2023-05-26 18' > slow_specifyTime.log

#### 掉盘和恢复命令

掉盘命令：/opt/MegaRAID/storcli/storcli64 /c0/e08/s08 set good force
e和s后面组合起来就是挂载路径data0808
恢复命令：/opt/MegaRAID/storcli/storcli64 /c0/e08/s08 set jbod
上述命令仅仅适用于宝德和华为标准服务器。

#### 批量kill进程

ps -ef | grep "dd if" | awk '{print $2}' | xargs kill

#### 查看某个进程的所有线程

pidstat -p 297389 –t

#### 制造大文件

fallocate -l 3227517016k ./test.tx

#### 安装java

yum install java
yum install java-devel  执行该命令后，jps等命令才能执行

#### 查询某个路径下有包含某内容的文件

```bash
grep -rl "日志文件.png" *
```

解释：

- `grep`：搜索工具
- `-r`：递归搜索子目录
- `-l`：只输出文件名（不会打印匹配行内容）

如果只想在当前目录，不递归，可以去掉 `-r`：

```bash
grep -l "日志文件.png" *
```

这样就能直接列出包含 `"日志文件.png"` 的文件名。

### hadoop相关

#### 修改日志级别

Yarn相关的修改yarn-daemon.sh脚本。

Hdfs相关的修改hadoop-daemon.sh。

jobhistory修改mr-jobhistory-daemon.sh脚本

#### 查询目录配额（总大小，文件和文件夹个数）

hdfs dfs -count -q -v -h /user/user4

#### 生成10000个10B大小的文件

hadoop jar hadoop-mapreduce-client-jobclient-2.10.1-HDP-22.04.1-snapshot-tests.jar TestDFSIO -write -nrFiles 10000 -size 10B

#### hdfs命令和源码的对应

hdfs dfs对应FsCommand类（-cat 对应Display类，-put和-copyFromLocal都对应CopyCommands类）

dfsadmin对应DFSAdmin类

haadmin对应DFSHAAdmin类

zkfc对应DFSZKFailoverController类

#### 统计datanode慢日志

egrep -o "Slow.*?(took|cost)" /path/to/current/datanode/log | sort | uniq -c

#### hdfs客户端加xmx参数

export HADOOP_CLIENT_OPTS="-Xmx20480m"

#### hdfs命令指定日志级别

hdfs --loglevel DEBUG haadmin –getAllServiceState

#### 触发全量块汇报

hdfs dfsadmin -triggerBlockReport 172.16.20.25:50020

#### 编译时执行某个测试类

mvn -pl ModuleName -Dtest=TestClassName#methodName clean install 
例如：mvn -pl org.apache.hadoop:hadoop-hdfs -Dtest=org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList#testExcludeSlowDiskWhenChoosingVolume clean install

#### 上传文件副本数为1的文件

 hdfs dfs  -Ddfs.replication=1  -put /function-hadoop.sh /

#### 将当前和后续某个目录下的文件的副本数设置为1

hdfs dfs -setrep -R 1 /hbase

#### 恢复仅block校验文件损坏

对于block校验文件损坏，可以通过hdfs debug computeMeta -block /cloud/data7/hadoop/dfs/dn/current/BP-403140199-172.16.57.159-1737598627537/current/finalized/subdir2/subdir17/blk_1073934702 -out /cloud/data7/hadoop/dfs/dn/current/BP-403140199-172.16.57.159-1737598627537/current/finalized/subdir2/subdir17/blk_1073934702_193946.meta重建恢复

#### 针对某个类开启debug日志

在log4j.properties文件中加入  log4j.logger.BlockStateChange=DEBUG
log4j.logger.org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager=DEBUG

#### 开启远程调试

1. 服务端配置
   修改服务器上的脚本/cloud/service/hadoop-2.10.1/sbin/hadoop-daemon.sh，在下面的位置加上远程调试环境变量：
  ```
  if [ -f "${HADOOP_CONF_DIR}/hadoop-env.sh" ]; then
    . "${HADOOP_CONF_DIR}/hadoop-env.sh"
    远程调试环境变量
  fi
  ```

  如果是rm，修改服务器上的脚本/cloud/service/hadoop-2.10.1/sbin/yarn-daemon.sh，在下面的位置加上远程调试环境变量：
  
  ```
  if [ -f "${YARN_CONF_DIR}/yarn-env.sh" ]; then
    . "${YARN_CONF_DIR}/yarn-env.sh"
    远程调试环境变量
  fi
  ```

| 组件     | 环境变量设置                                                 |
| -------- | ------------------------------------------------------------ |
| nn       | export HADOOP_NAMENODE_OPTS="-agentlib:jdwp=transport=dt_socket,address=5005,server=y,suspend=n $HADOOP_NAMENODE_OPTS" |
| dn       | export HADOOP_DATANODE_OPTS="-agentlib:jdwp=transport=dt_socket,address=9888,server=y,suspend=n $HADOOP_DATANODE_OPTS" |
| rm       | export YARN_RESOURCEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,address=10888,server=y,suspend=n $YARN_RESOURCEMANAGER_OPTS" |
| nm       | export YARN_NODEMANAGER_OPTS="-agentlib:jdwp=transport=dt_socket,address=10888,server=y,suspend=n $YARN_NODEMANAGER_OPTS" |
| timeline | export YARN_TIMELINESERVER_OPTS="-agentlib:jdwp=transport=dt_socket,address=11888,server=y,suspend=n $YARN_TIMELINESERVER_OPTS" |

*注：后面必须加上$HADOOP_DATANODE_OPTS（2022.10.22改，在dn调试时发现该问题，若不加该参数，则jvm的一些参数会丢失，如本来指定了Xmx为5600m，结果变成默认的2048m。但是加了该参数后，由于hadoop-env.sh不止调用一次，会出现3个-agentlib参数，不支持会报错。）

2. idea端配置

  ```
  export HADOOP_OPTS="-agentlib:jdwp=transport=dt_socket,address=5005,server=y,suspend=y $HADOOP_OPTS"
  ```



### kafka相关


#### 9090端口生产

1. export KAFKA_OPTS="-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/cloud/service/kafka/config/jaas.conf"
2. 创建文件client.properties，内容如下：
   security.protocol=SASL_PLAINTEXT
   sasl.kerberos.service.name=kafka
3. 接着执行如下命令即可通过9090生产
   kafka-producer-perf-test.sh --topic test1 --num-records 100000000000000 --record-size 10240 --throughput 10000000 --producer-props bootstrap.servers=192.168.181.238:9090 --producer.config ./client.properties

如果在物理机上运行生产消费，则还需要在/etc/hosts中加入192.168.149.219 hdp-kafka-hdp-kafka-0.hdp-kafka-hdp-kafka.kafka-perf-test.svc.cluster.local     hdp-kafka-hdp-kafka-0。否则kdc认证会有问题，因为nslookup会返回hdp-kafka-hdp-kafka-0.hdp-kafka-hdp-kafka-0.kafka-perf-test.svc.cluster.local和hdp-kafka-hdp-kafka-0.hdp-kafka-hdp-kafka.kafka-perf-test.svc.cluster.local，顺序不一定，如果hdp-kafka-hdp-kafka-0.hdp-kafka-hdp-kafka0.kafka-perf-test.svc.cluster.local先返回则认证失败

Nodeport如果要开启kerberos鉴权，则需要在客户端/etc/hosts中配置主机ip和对应kafka pod域名之间的映射关系。因为在进行认证的时候，会根据ip反解析域名，如果不配置，则会反解析出主机的域名，kerberos认证会失败。

#### 限速

a.	消费限速（每个broker 2/3GB/s，3个broker整体消费速度为2GB/s）：kafka-configs.sh --zookeeper 192.168.181.233:2181/kafka --alter --add-config 'consumer_byte_rate=715827882' --entity-type clients --entity-name clientA
查看限速：kafka-configs.sh --zookeeper 192.168.181.233:2181/kafka --describe --entity-type clients --entity-name clientA
取消限速：kafka-configs.sh --zookeeper 192.168.181.233:2181/kafka --alter --entity-type clients --entity-name clientA --delete-config 'consumer_byte_rate'

#### 让首副本成为主分区

b.	让首副本成为主分区：kafka-preferred-replica-election.sh --zookeeper 192.168.181.233:2181/kafka

#### 设置某个topic的生命周期

c.	设置test1主题的消息保存时间为2小时：kafka-configs.sh --zookeeper 192.168.181.233:2181/kafka --alter --entity-type topics --entity-name test1 --add-config 'retention.ms=7200000'

#### kafka社区jira地址

https://issues.apache.org/jira/browse/KAFKA-19460

#### 开启远程调试

修改/cloud/service/kafka/bin/kafka-run-class.sh文件

加入：

```
export KAFKA_DEBUG=xxx
export DEBUG_SUSPEND_FLAG=y
```
在脚本内部会自动将调试端口设置为5005：DEFAULT_JAVA_DEBUG_PORT="5005"。在容器内执行脚本也会走kafka-run-class.sh文件，也会尝试在5005上开启调试，报错。需要在执行命令之前将这两行注释掉。



### spark

spark-submit --master yarn --deploy-mode cluster --class org.apache.spark.examples.JavaSparkPi /cloud/service/spark/examples/jars/spark-examples_2.11-2.4.4-HDP-22.05.30.snapshot.1.jar

### flink

flink run -m yarn-cluster /cloud/service/flink/examples/batch/WordCount.jar

### hive

beeline -u 'jdbc:hive2://192.168.156.20:10000/;principal=hadoop/hdp-hive-hdp-hive-server2-0.hdp-hive-hdp-hive-server2.testns.svc.cluster.local@DAHUA.COM'

### kerberos

kdestroy
klist
kinit hadoop
klist -ket /cloud/data/hadoop/configcenter/Kerberos/Hdfs_NameNode.keytab
kinit -kt /cloud/data/hadoop/configcenter/Kerberos/Hdfs_NameNode.keytab hadoop

#### jks文件

查看文件内容
keytool -list -keystore /cloud/service/hadoop/etc/hadoop/keystore.jks
keytool -list -keystore /cloud/service/hadoop/etc/hadoop/truststore.jks

### arthas

watch org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer doSaslHandshake {params[3]} -x 2
default value {params, target, returnObj}
-e throwExp
-b是在方法前调用（如果加了returnObj，则默认在方法后调用了）
默认获得100个调用后就退出，可以添加-n指定次数
指定条件：watch org.apache.hadoop.hdfs.server.datanode.BlockSender sendPacket "{params}" "params[3]==true"
watch kafka.server.SensorAccess getOrCreate {params[0],returnObj.stats.get(0).stat.samples} 'params[0].equals("test_6produce rate")' -x 1 -n 1

arthas默认不能watch系统级别的类，如watch sun.net.www.protocol.http.HttpURLConnection getInputStream。可以通过将变量unsafe修改成true查看(如通过options查看所有选项，通过options save-result true保存结果)

查看类所在的jar包：sc -d com.sun.jersey.client.urlconnection.URLConnectionClientHandler

trace 可以看方法内调用耗时（还可以看方法内那些代码被调用了）
stack可以看调用该方法的堆栈

### 杂

#### 专利检索

https://analytics.zhihuiya.com/search/input#/simple
账户：dh_pat6@dahuatech.com
密码：dh6@2021

#### ranger密码

dahua@6E9wVzJCba

#### 重置控制台admin密码

update `USER` set `PASSWORD` = '0854bf276b3be3d798bda2e0144961ee', STATUS = 1 where NAME = 'admin';

#### agent执行命令

supervisorctl containerpipe /cloud/dahua/adapter/shell/info kafka
supervisorctl containerpipe /cloud/dahua/adapter/shell/extend

#### 线上容器云测试用户密码

hdp-tester/hdp-test@123
DH3.RDA000022为计算平台
DH3.RDA000020为基础组件

#### 默认镜像密码

默认镜像的密码为dahuacloud
