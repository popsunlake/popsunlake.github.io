 

 

 

Kafka同步复制吞吐优化

 

 

 

 

**修订历史**

| **编号** | **修订内容描述** | **修订人** | **修订日期**   | **修订后版本号** |
| -------- | ---------------- | ---------- | -------------- | ---------------- |
| 1        | 创建文件         | **陈文灿** | **2025.06.23** | **V1.0.0**       |
| 2        |                  |            |                |                  |
| 3        |                  |            |                |                  |

 



 

# 问题背景与现象

## 问题背景

在分布式系统中，消息的可靠传递是至关重要的，由于网络异常、节点故障等原因，消息在传输过程中可能出现丢失或重复处理。为了确保消息的可靠传输，kafka引入了消息确认机制，该机制保证了消息从生产者到服务端的可靠传输。

当生产者发送消息到kafka集群时，可以设置不同的acks参数来控制消息发送后的确认机制，共有三种确认模式：

1)   acks=0：生产者发送消息后不需要等待任何Broker的响应，即只要消息发送，即使该消息因为网络异常未正确到达服务端，又或者服务端接收到请求后没有成功持久化到本地，也都认为是成功的。显然，这种默认没有任何可靠性。

2)   acks=1：生产者需要等待接收到leader成功将消息写入本地后才返回的响应，才认定是成功发送的。这种模式下，提供了一定消息可靠性，因为副本已经写入到leader了。但如果leader还未同步给follower之前出现了异常（比如磁盘出现损坏），那么此时消息仍然是会丢失的。

3)   acks=-1：生产者必须等待接收到所有在ISR中的副本都成功写入本地后才返回的响应，才认定消息成功发送。在这种模式下，由于需要等待所有副本均写入成功后才进行响应，因此具备较高的可靠性，该模式也被称为同步复制模式。

## 问题现象

在实际使用中，由于acks=0完全不具备可靠性，因此一般采用acks=1或者acks=-1两种模式。然而，生产者的吞吐量在这两种模式下有着非常大的差别，如下图所示（单个topic单个分区）：

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image002.jpg)

图1 两种acks模式的吞吐对比

如果采用acks=1的模式，那么就得承受消息可能丢失的风险，而如果采用acks=-1的模式，要达到符合业务场景的吞吐量，需要放大topic的分区数，这也就意味着需要更大的集群规模，整体成本自然也就提高了。

那么高吞吐和高可靠是否可以兼得，本文就针对acks=-1时，生产吞吐非常低的问题进行深入剖析。

# 问题涉及的基础知识

## 生产者消息发送流程

简单概括生产者消息发送的逻辑就是：

1)   业务线程调用producer.send()后，内部将消息进行序列化，并缓存到内存中，然后按需唤醒sender线程进行发送。

2)   sender发送线程从内存中挑选出待发送的消息集合，并按照指定协议格式构造生产请求，然后发送给topic分区leader对应的broker，再接收broker的请求响应，并进行响应的处理以及回调通知。此后不断循环这个过程。

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image004.jpg)

图2 客户端消息发送逻辑

注意在往broker发送请求时有一个小细节：我们知道不同topic分区的leader可能位于不同的broker上，因此客户端会与对应的broker建立tcp连接，然后分别发送消息。而对于每个tcp连接（也就是每个broker）都有一个队列，sender线程在进行消息发送前，先将消息记录到队列中，然后再进行发送；当接收到服务端的请求响应时，再将请求从队列中移除。到下一次轮询时，会先判断对应连接的队列中的请求个数是否小于队列的最大长度（队列最大长度可配置，默认为5），如果不满足条件，则跳过往对应broker发送消息，直到收到来自该broker的请求响应，从队列中移除请求后，才能继续向该broker发送请求消息。

## 服务端处理流程

服务端的架构可以简单分为网络线程组和消息处理线程组（也可以成为IO线程，后续都称之为IO线程）两大块。网络线程负责socket轮询接收客户端的请求，以及将请求响应通过socket向客户端进行发送；而IO线程则负责消息的实际处理，比如将生产者请求的消息持久化写入本地，或者对于消费者fetch请求从本地持久化文件读取对应的消息。

网络线程从socket接收到完整请求后，将请求放到一个公共的请求队列中；多个IO线程从请求队列中抢占获取请求并进行实际的处理；当请求处理完毕后，将请求的响应放到请求对应的网络线程的响应队列中；最后，网络线程从响应队列中取出请求响应，发送给客户端。

以生产者请求为例，完整的处理流程如下图所示：

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image006.jpg)

图3 服务端生产请求处理时序

从图中可以看到，IO线程在完成生产者请求的处理后，会判断生产请求的ACK模式，如果为异步复制模式（acks=1），那么直接构造请求响应并添加到网络线程的请求响应队列中；如果为同步模式（acks=-1），那么就需要缓存该请求，延迟直到follower完成该请求的处理后才进行响应。

# 问题分析

## leader或follower处理太慢？

了解了生产者请求的发送与处理过程之后，我们再回过头来分析这个问题。

首先，怀疑是follower对请求消息的处理耗时较长，导致生产者请求响应耗时变长，从而影响了生产者的发送速度。然而，在服务端统计的生产者请求耗时来看，在同步复制模式下，99%的生产请求其耗时都小于等于1ms，99%到99.9%之间才会出现一些波动。

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image008.jpg)

图4 99%的生产请求处理耗时

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image010.jpg)

图5 99.9%的生产请求处理耗时

也就是说，只有极个别的请求处理耗时偏大，但这不至于对生产者发送速度造成非常大的影响。同时，对比异步复制模式下，生产者的吞吐流量，以及follower从leader消息拉取的吞吐流量，两者几乎持平，也就是说follower的处理耗时不是瓶颈。

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image012.jpg)

图6 acks=1模式下生产者流量与follower拉取流量

因此，基本排除是leader或follower处理慢引发的问题。

## 生产者的队列最大长度太小？

既然生产请求的响应是比较快的，那为什么客户端的发送速度还是上不去。另外，即便是响应有一定的延迟，生产者也应该可以继续进行生产请求的发送。因此，怀疑是否是因为客户端发送队列的最大长度太小，即等待响应的请求数达到队列最大长度，导致sender线程轮询时不会再次进行请求的发送。

通过将队列长度（配置为max.in.flight.requests.per.connection）最大值修改为2000后，再次进行测试，其吞吐并没有任何改变，甚至通过arthas持续查看该队列的值，发现实际队列长度最大都没有超过20。

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image014.jpg)

图7 队列实际长度

## 根因所在！

既然队列长度不是影响吞吐的必要条件，那么猜测是否存在tcp窗口持续为0，引起客户端无法发送（或者发送接口被阻塞），从而导致吞吐上不去。

再次测试并抓包分析，发现确实有较长时间且很频繁的出现“tcp zero window”的情况。

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image016.jpg)

图8 生产过程中的tcp zero window

证实情况后，再进一步进行分析，这种情况大概率是因为服务端网络线程处理慢，导致消息持续在服务端socket的接收缓冲区中未被读取，从而出现“tcp zero window”的情况，继而拉慢了生产者的发送速度。

但是，对比异步复制的吞吐来看，网络线程对于这点吞吐应该不至于处理不过来，除非是哪里出现了阻塞才导致出现问题。

带着疑问，再次走读服务端网络线程相关的代码，终于发现了问题：简单来说就是，从同一个socket上接收完一个请求之后，必须等待这个请求响应发送之后，才继续从socket的接收缓冲区中读取数据进行下一个请求的处理。

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image017.jpg)

图9 服务端网络线程收发流程

思考：为什么需要采用这样一问一答的串行处理方式呢？

因为在网络线程中，请求被添加到请求队列之后，后续是并行有多个IO处理线程同时从队列中获取请求进行处理的，为了保证同一个连接的请求是有序被处理的，故采取了这样的逻辑。

同时，这里也就能解释，为什么服务端99%的生产请求耗时均小于等于1ms了， 因为客户端发送的请求持续积压在socket的接收缓冲区中，还未被网络线程读取，压根不会计入耗时统计中。

# 问题优化方案

首先想到的优化办法是：使用多个客户端（句柄）进行发送，这样虽然每个客户端都会受到限制，但是多个客户端能使吞吐线性递增并稳定到一个较高的值。经过实测也证实采用多个客户端进行发送，同步复制的吞吐可以达到异步复制的吞吐。

但是，这种方式对于业务使用有一定的改动，比如说需要业务侧在同一个进程中创建多个客户端句柄进行发送；同时对于有序的消息需要使用同一个句柄进行发送，即消息的有序性需要业务测通过编写相关代码来保证。

另外，在实际使用过程中，kafka可能会与第三方对接，那么要求第三方按照这个逻辑进行修改是很难推动的。因此这个优化虽然可以达到效果，但不具备可落地性。

再深入思考，对于同步复制的生产者请求，如果在IO线程中完成消息的处理后，等待follower处理的同时，是否可以回调通知网络线程解除请求对应tcp连接socket的阻塞，即允许网络线程继续接收请求进行后续处理，如此一来，对于leader而言，请求在IO线程中依旧是串行处理的；而对于follower而言，fetch请求的处理逻辑不变，fetch到的消息也同样是有序的。解决了消息处理有序的同时，对于网络线程而言加速了请求的接收处理。

## 优化逻辑

1)   网络线程中状态机的优化变更

在kafka的网络层代码中，每个tcp连接都有一个kafkaChannel的实例对象与之对应，在kafkaChannel中通过状态机来对应请求接收、请求响应发送的逻辑。

状态机的初始状态与变化为：

a)  连接建立后，状态机初始为not_muted；

b)  当从socket接收到完整请求后，将状态置为muted，此后将请求添加到队列中，然后发送request_receive事件，该事件会触发状态机发生变化，进入muted_and_response_pending状态，即等待请求响应；

c)   当网络线程从响应队列获取到请求响应并成功发送后，发送response_send事件，触发状态机变为muted状态，接着在调用unmute接口，将状态变更为not_muted状态。

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image018.jpg)

图10 原生状态机

上述状态机变化就是一个请求从接收到响应发送的全过程。在这个过程中，还有一个小细节需要注意：连接的状态机在变化的过程中，会伴随着对socket可读可写事件的添加和移除，即接收到一个请求之后，移除socket的可读事件；确保不会从socket接收缓冲区中继续读取客户端的后续请求内容；等到响应成功发送之后，再重新添加socket的可读事件，这样也就保证了同一个连接中的请求严格按顺序进行处理。

了解了原生逻辑后，可以针对状态机做如下变化：

a)  增加一个状态reqcached_and_response_pending，标识生产请求在本地（leader）已完成处理。

b)  当接收到request_receive事件后仍旧进入muted_and_response_pending状态，此后当IO线程完成本地处理（持久化写入）后，向网络线程发送request_local_processed事件，状态转换为reqcached_and_response_pending。

c)   此后，当请求的响应成功发送后，即response事件触发状态从reqcached_and_response_pending转换到muted。

到这里，是单个请求完整的处理流程，在原有基础上新增了一个状态以及对应的触发事件。

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image019.jpg)

图11 增加本地处理后的状态机

考虑到在reqcached_and_response_pending状态下，针对该tcp连接，网络线程允许接收新的请求，因此还需要再增加一个状态（reqprocessing_and_reqcached_and_response_pending）,标识有请求本地已处理正在等待响应，同时又接收到了新的请求，并且该请求还未完成本地处理。

a)   当状态切换到reqcached_and_response_pending，此时如果先接收到新的请求，即收到request_received事件后，状态需要转换到reqprocessing_and_reqcached_and_response_pending。

b)   此后再收到请求本地完成处理的事件时，状态重新转换到reqcached_and_response_pending，即当前所有请求均已完成本地处理

由于一个请求在本地完成处理之后还未得到响应之前，允许网络线程继续从socket读取下一个请求并进行处理，那么就可能存在同一个连接的多个请求，在本地均已完成处理并且等待响应。

那么在新增的两个状态中，接收到请求响应已发送事件（response_sent）时，需要做一些额外的判断才能进行状态的变更，例如：

a)   在reqcached_and_response_pending状态下，此时可能缓存了多个已完成处理的请求，当最早缓存的请求响应成功发送后，状态应该保持不变而不是直接切换到muted状态，直到所有缓存的请求其响应全部成功发送之后，才能切换到muted状态。

b)   同理，在reqprocessing_and_reqcached_and_response_pending状态下，只有当所有缓存请求的响应都发送成功之后，才转换到muted_and_response_pending状态，否则状态不发生变化。

因此，完整的状态机变化如下图所示：

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image021.jpg)

图12 优化后的完整状态机

2)   网络线程中接收发送的逻辑变更

在原有代码中，kafkaChannel中仅保存一个待发送的请求响应内容，变量初始为空，当IO线程处理完成后对其进行赋值；当请求响应成功发送之后，重新置为空（null）。

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image023.jpg)

图13 原生的请求响应保存与发送逻辑

而根据优化后的状态机可以知道，在kafkaChannel中可能缓存多个本地已处理的请求，那也就意味着可能同时有多个待发送的请求响应，因此这里需要保存多个请求响应。

由于本地已处理的请求在缓存后，可能存在后处理的请求，其请求响应先到达网络线程的请求响应队列中（比如先处理客户端发送的生产请求，接着处理客户端元数据更新请求，而元数据更新请求不需要等待follow的处理而直接响应，因此后到的元数据请求响应会先于生产请求响应到达网络线程中），为了确保给客户端应答时仍旧按照请求达到的先后顺序进行响应，因此接收到本地处理完成的事件后，首先需要将请求缓存到先进先出的map中（key为请求的递增ID，value为null），当收到请求响应后，再根据请求ID更新map中的value，此后等待请求响应的发送。优化后的逻辑为：

![img](file:///C:/Users/233662/AppData/Local/Temp/msohtmlclip1/01/clip_image025.jpg)

图14 优化后的请求响应保存与发送逻辑

## 优化效果

在半山环境中进行了如下对比测试（单个topic，分区数为1，副本为2，生产者单条消息大小为10KB）

|              | ack=-1 | ack=1   | 单副本  |
| ------------ | ------ | ------- | ------- |
| 优化前的吞吐 | 16MB/s | 110MB/s | 115MB/s |
| 优化后的吞吐 | 48MB/s | 110MB/s | 120MB/s |

在未进行参数调优的情况下，ack=-1在优化后的吞吐比优化前有了明显的提升， 但是距离ack=1仍旧有一定的差距。

将客户端参数max.in.flight.requests.per.connection从5调整到200后，再次进行测试，对比数据如下所示：

|              | ack=-1  | ack=1   | 单副本  |
| ------------ | ------- | ------- | ------- |
| 优化前的吞吐 | 21MB/s  | 110MB/s | 115MB/s |
| 优化后的吞吐 | 140MB/s | 110MB/s | 115MB/s |

可以看到，此时优化后极大的提升了ack=-1时的吞吐。
